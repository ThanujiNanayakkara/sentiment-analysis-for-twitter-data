{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "id": "zuNTMvLsdSE6"
      },
      "source": [
        "# Question 4: Optimising pre-processing and feature extraction (50 marks)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_iN2zOfhdSFB"
      },
      "source": [
        "**Note:** it is advisable to implement question 4 in a separate notebook where you further develop the pre-processing and feature extraction functions you implemented above."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 213,
      "metadata": {
        "id": "ZrQ6MBNzdSE8"
      },
      "outputs": [],
      "source": [
        "import csv                               # csv reader\n",
        "from sklearn.svm import LinearSVC\n",
        "from nltk.classify import SklearnClassifier\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import precision_recall_fscore_support # to report on precision and recall\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from random import Random"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 214,
      "metadata": {
        "id": "sRg7VOMLdSE-"
      },
      "outputs": [],
      "source": [
        "def load_data(path):\n",
        "    \"\"\"Load data from a tab-separated file and append it to raw_data.\"\"\"\n",
        "    with open(path) as f:\n",
        "        reader = csv.reader(f, delimiter='\\t')\n",
        "        for line in reader:\n",
        "            (label, text) = parse_data_line(line)\n",
        "            raw_data.append((text, label))\n",
        "\n",
        "\n",
        "def split_and_preprocess_data(percentage):\n",
        "    num_samples = len(raw_data)\n",
        "    n_train = int((percentage * num_samples))\n",
        "    for (text, label) in raw_data[:n_train]:\n",
        "        train_data.append((text, label))\n",
        "    for (text, label) in raw_data[n_train:]:\n",
        "        test_data.append((text, label))\n",
        "\n",
        "def parse_data_line(data_line):\n",
        "    \"\"\"Return a tuple of the label as just FAKE or REAL and the statement\"\"\"\n",
        "    return (data_line[1], data_line[2])\n",
        "\n",
        "def pre_process(text):\n",
        "    \"\"\"Return a list of tokens\"\"\"\n",
        "    return text.split()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 215,
      "metadata": {
        "id": "RnYi-3nEdSE_"
      },
      "outputs": [],
      "source": [
        "#solution\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import precision_recall_fscore_support, accuracy_score\n",
        "from collections import Counter\n",
        "\n",
        "labels_hm = ['positive', 'negative']\n",
        "\n",
        "\n",
        "def cross_validate(dataset, folds, classifier_fn):\n",
        "    \"\"\"\n",
        "      Manual K-fold cross-validation over the *training* data.\n",
        "\n",
        "      Parameters\n",
        "      ----------\n",
        "      dataset : list[tuple[dict, str]]\n",
        "          List of (feature_dict, label) pairs.\n",
        "      folds : int\n",
        "          Number of folds (e.g., 10).\n",
        "\n",
        "      Returns\n",
        "      -------\n",
        "      tuple(float, float, float, float)\n",
        "          Average (precision, recall, F1, accuracy) across folds.\n",
        "      \"\"\"\n",
        "\n",
        "    results = []\n",
        "    fold_size = int(len(dataset)/folds) + 1\n",
        "    # Leave-out-chunk CV\n",
        "    for i in range(0,len(dataset),int(fold_size)):\n",
        "        # insert code here that trains and tests on the 10 folds of data in the dataset\n",
        "\n",
        "        print(\"Fold start on items %d - %d\" % (i, i+fold_size))\n",
        "        # FILL IN THE METHOD HERE\n",
        "        # Split current chunk as validation; rest as training\n",
        "        test_fold = dataset[i:i+fold_size]\n",
        "        train_fold = dataset[:i] + dataset[i+fold_size:]\n",
        "\n",
        "        classifier = classifier_fn(train_fold)\n",
        "\n",
        "        # Prepare features/labels for validation and predict\n",
        "        test_features = [features for (features, label) in test_fold]\n",
        "        true_labels = [label for (features, label) in test_fold]\n",
        "\n",
        "        predicted_labels = predict_labels(test_features, classifier)\n",
        "\n",
        "        # Compute standard metrics (weighted to handle any class imbalance)\n",
        "        precision, recall, f1, _ = precision_recall_fscore_support(\n",
        "            true_labels, predicted_labels, average='weighted', zero_division=0\n",
        "        )\n",
        "        accuracy = accuracy_score(true_labels, predicted_labels)\n",
        "\n",
        "        # Store this fold's metrics\n",
        "        results.append((precision, recall, f1, accuracy))\n",
        "\n",
        "        # Save the first fold's outputs for Q3 analysis/plots\n",
        "\n",
        "    # Average metrics across all folds\n",
        "    cv_results = tuple(float(np.mean([r[k] for r in results])) for k in range(4))\n",
        "    print(\"\\nAverage Precision: %.4f\\nAverage Recall: %.4f\\nAverage F1: %.4f\\nAverage Accuracy: %.4f\" %\n",
        "          tuple(cv_results))\n",
        "\n",
        "    return cv_results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 216,
      "metadata": {
        "id": "jXaAPNIXdSFA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0c1ec833-e872-4145-f444-de902696cb8d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Now 0 rawData, 0 trainData, 0 testData\n",
            "Preparing the dataset...\n",
            "Now 33540 rawData, 0 trainData, 0 testData\n",
            "Preparing training and test data...\n",
            "After split, 33540 rawData, 26832 trainData, 6708 testData\n",
            "Training Samples: 26832\n"
          ]
        }
      ],
      "source": [
        "# MAIN\n",
        "\n",
        "# loading reviews\n",
        "# initialize global lists that will be appended to by the methods below\n",
        "raw_data = []          # the filtered data from the dataset file\n",
        "train_data = []        # the pre-processed training data as a percentage of the total dataset\n",
        "test_data = []         # the pre-processed test data as a percentage of the total dataset\n",
        "\n",
        "\n",
        "# references to the data files\n",
        "data_file_path = 'sentiment-dataset.tsv'\n",
        "\n",
        "# Do the actual stuff (i.e. call the functions we've made)\n",
        "# We parse the dataset and put it in a raw data list\n",
        "print(\"Now %d rawData, %d trainData, %d testData\" % (len(raw_data), len(train_data), len(test_data)),\n",
        "      \"Preparing the dataset...\",sep='\\n')\n",
        "\n",
        "load_data(data_file_path)\n",
        "\n",
        "# from collections import Counter\n",
        "\n",
        "# # raw_data = [(text1, label1), (text2, label2), ...]\n",
        "\n",
        "# labels = [label for (_, label) in raw_data]\n",
        "# label_counts = Counter(labels)\n",
        "\n",
        "# Data shuffling to avoid data imbalance among the folds during cross validation\n",
        "rng = Random(42)\n",
        "shuffled = raw_data[:]\n",
        "rng.shuffle(shuffled)\n",
        "raw_data = shuffled\n",
        "\n",
        "# print(label_counts)\n",
        "# We split the raw dataset into a set of training data and a set of test data (80/20)\n",
        "# You do the cross validation on the 80% (training data)\n",
        "# We print the number of training samples and the number of features before the split\n",
        "print(\"Now %d rawData, %d trainData, %d testData\" % (len(raw_data), len(train_data), len(test_data)),\n",
        "      \"Preparing training and test data...\",sep='\\n')\n",
        "\n",
        "split_and_preprocess_data(0.8)\n",
        "\n",
        "# train_texts = [t for (t, _) in train_data]\n",
        "# vec_probe = CountVectorizer(\n",
        "#             tokenizer=tokenize_lemmatize,\n",
        "#             lowercase=False,\n",
        "#             ngram_range=(1,2),\n",
        "#             binary=True,\n",
        "#         )\n",
        "# _ = vec_probe.fit_transform(train_texts)\n",
        "# vocab_size = len(vec_probe.get_feature_names_out())\n",
        "\n",
        "# We print the number of training samples and the number of features after the split\n",
        "print(\n",
        "    f\"After split, {len(raw_data)} rawData, {len(train_data)} trainData, {len(test_data)} testData\",\n",
        "    f\"Training Samples: {len(train_data)}\",\n",
        "    # f\"Vectorizer vocab (train-only): {vocab_size}\",\n",
        "    sep='\\n'\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 1 & 2 of Processing - Improve the preprocessing. Which tokens might you want to throw out or preserve? What about punctuation? Do not forget normalisation, lemmatising, stop word removal - what aspects of this might be useful?"
      ],
      "metadata": {
        "id": "9K5p0d1HnCsc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 217,
      "metadata": {
        "id": "a18b612a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "50543b98-0ea9-4527-a6ce-fad4afbcdd7d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# === Tokenizer & preprocessing helpers ===\n",
        "import re\n",
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
        "from functools import lru_cache\n",
        "\n",
        "# Download required WordNet data for lemmatization\n",
        "nltk.download('wordnet')\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# Keep negations in stopwords (they matter for sentiment)\n",
        "NEGATIONS = {\"no\", \"not\", \"nor\", \"never\", \"n't\"}\n",
        "STOP_SK = set(ENGLISH_STOP_WORDS) - NEGATIONS\n",
        "\n",
        "# Regular expressions for common text patterns\n",
        "URL_RE       = re.compile(r'(https?://\\S+|www\\.\\S+)', re.IGNORECASE)\n",
        "USER_RE      = re.compile(r'@\\w+')\n",
        "HASHTAG_RE   = re.compile(r'#(\\w+)')\n",
        "NUM_RE       = re.compile(r'(?<!\\w)(\\d+([.,]\\d+)*)(?!\\w)')\n",
        "ELONG_RE     = re.compile(r'(.)\\1{2,}', re.IGNORECASE)  # e.g., \"soooo\" -> \"soo\"\n",
        "DOTS_RE      = re.compile(r'\\.{3,}')                   # ellipsis \"...\"\n",
        "RETWEET_RE   = re.compile(r'^RT\\s')\n",
        "PUNCT_PATTERN = re.compile(r'[^\\w\\s!?\\.<>]')            # keep \"!\", \"?\" for sentiment\n",
        "EMOTICON_RE  = r'[:;=8][\\-^]?[)D(\\]/\\\\OpP]'             # :) :( :D ;-) etc.\n",
        "\n",
        "# Tokenizer splits words, emojis, tags, and punctuation\n",
        "TOKEN_SPLIT = re.compile(\n",
        "    r\"<[^<>]+>|\"                # <URL>, <USER>, etc.\n",
        "    r\"(?:%s)|\" % EMOTICON_RE +  # emoticons\n",
        "    r\"\\w+[:']\\w+|\\w+|\"          # words (incl. \"it's\")\n",
        "    r\"[^\\w\\s]\"                  # leftover punctuation\n",
        ")\n",
        "\n",
        "# Emoticon set for quick lookup\n",
        "EMOTICONS = {\":)\",\":-)\",\":D\",\":-D\",\":(\",\":-(\",\";)\",\";-)\",\":P\",\":-P\",\":'(\",\"XD\",\"xD\",\":-|\",\":/\",\":-/\"}\n",
        "\n",
        "\n",
        "@lru_cache(maxsize=200_000)\n",
        "def lemma_cached(token: str) -> str:\n",
        "    \"\"\"Lemmatize a token with caching for speed.\"\"\"\n",
        "    t = lemmatizer.lemmatize(token, 'v')  # try verb\n",
        "    if t == token:\n",
        "        t = lemmatizer.lemmatize(token, 'n')  # fallback to noun\n",
        "    return t\n",
        "\n",
        "\n",
        "def tokenize_lemmatize(text: str) -> list[str]:\n",
        "    \"\"\"\n",
        "    Custom tweet tokenizer:\n",
        "    - Replaces URLs, mentions, numbers, retweets with markers\n",
        "    - Handles hashtags, repeated characters, ellipses\n",
        "    - Keeps key punctuation and emoticons\n",
        "    - Lowercases, strips punctuation, lemmatizes long alphabetic tokens\n",
        "    \"\"\"\n",
        "\n",
        "    # Replace common patterns with markers\n",
        "    #1st step pf pre-processing\n",
        "    text = RETWEET_RE.sub(' <RT> ', text)\n",
        "    text = URL_RE.sub(' <URL> ', text)\n",
        "    text = USER_RE.sub(' <USER> ', text)\n",
        "    text = NUM_RE.sub(' <NUM> ', text)\n",
        "    text = HASHTAG_RE.sub(lambda m: ' ' + m.group(1) + ' ', text)\n",
        "\n",
        "    #2nd step pf pre-processing\n",
        "    text = ELONG_RE.sub(r\"\\1\\1\", text)   # shorten long repeated chars\n",
        "    text = DOTS_RE.sub(' ... ', text)    # normalize ellipsis\n",
        "\n",
        "    raw = TOKEN_SPLIT.findall(text)\n",
        "    tokens, add_allcaps = [], False\n",
        "\n",
        "    for tok in raw:\n",
        "        if not tok.strip():\n",
        "            continue\n",
        "\n",
        "        # Keep emoticons and certain punctuation as-is\n",
        "        if tok in EMOTICONS or tok in {\"!\", \"?\", \"...\"}:\n",
        "            tokens.append(tok)\n",
        "            continue\n",
        "\n",
        "        # Remove punctuation (except inside markers)\n",
        "        if not (tok.startswith('<') and tok.endswith('>')):\n",
        "            tok = PUNCT_PATTERN.sub('', tok)\n",
        "            if not tok:\n",
        "                continue\n",
        "\n",
        "        # Flag for all-caps emphasis\n",
        "        if tok.isalpha() and tok.isupper() and len(tok) >= 2:\n",
        "            add_allcaps = True\n",
        "\n",
        "        # Lowercase and lemmatize normal words\n",
        "        if tok not in {\"<URL>\", \"<USER>\", \"<NUM>\", \"<RT>\"}:\n",
        "            tok = tok.lower()\n",
        "        #Remove stopwords\n",
        "        if tok in STOP_SK:\n",
        "          continue\n",
        "        if tok.isalpha() and len(tok) > 2:\n",
        "            tok = lemma_cached(tok)\n",
        "\n",
        "        tokens.append(tok)\n",
        "\n",
        "    # Append special token if tweet had ALL CAPS words\n",
        "    if add_allcaps:\n",
        "        tokens.append(\"__ALLCAPS__\")\n",
        "\n",
        "    return tokens\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 218,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tF6RYaHIdSE-",
        "outputId": "0a469288-4aca-443e-e822-a05d86b32c89",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['<RT>',\n",
              " '<USER>',\n",
              " ';-)',\n",
              " 'happy',\n",
              " 'bloody',\n",
              " 'instant',\n",
              " 'restaurant',\n",
              " 'week',\n",
              " '?',\n",
              " '!',\n",
              " '?',\n",
              " '!',\n",
              " 'seriously',\n",
              " '!',\n",
              " 'just',\n",
              " 'jump',\n",
              " 'shark',\n",
              " 'rid',\n",
              " 'shark',\n",
              " 'power',\n",
              " 'sh',\n",
              " '__ALLCAPS__']"
            ]
          },
          "metadata": {},
          "execution_count": 218
        }
      ],
      "source": [
        "# test\n",
        "text = \"RT @colonelkickhead: Another ;-) HAPPY bloody instant restaurant week?!?! Seriously! They just jumped the shark riding two other sharks powered by sh…\"\n",
        "tokenize_lemmatize(text)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 1, 2 & 3 of Processing Combined"
      ],
      "metadata": {
        "id": "uaBRq0mTnNfQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 219,
      "metadata": {
        "id": "PpGqCPBcdSE_"
      },
      "outputs": [],
      "source": [
        "#Step 3 Processing\n",
        "# TRAINING AND VALIDATING OUR CLASSIFIER\n",
        "def train_classifier(data):\n",
        "    print(\"Training Classifier...\")\n",
        "    X = [x for (x, _) in data]\n",
        "    y = [y for (_, y) in data]\n",
        "    model = Pipeline([\n",
        "        ('vect', CountVectorizer(\n",
        "            # tokenizer=pre_process,\n",
        "            tokenizer=tokenize_lemmatize,\n",
        "            lowercase=False,\n",
        "            # ngram_range=(1, 1),\n",
        "            ngram_range=(1,2),\n",
        "            binary=True,\n",
        "        )),\n",
        "        ('svc', LinearSVC(random_state=42, class_weight='balanced')),\n",
        "    ])\n",
        "    model.fit(X, y)\n",
        "    return model\n",
        "\n",
        "def predict_labels(samples, classifier):\n",
        "    return classifier.predict(samples)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 220,
      "metadata": {
        "id": "ahYA4CAGdSFA",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "# cross_validate(train_data, folds=10, classifier_fn=train_classifier) # will work and output overall performance of p, r, f-score when cv implemented\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 3 of Processing - Think about the features: what could you use other than unigram tokens? It may be useful to look beyond single words to combinations of words or characters. Also the feature weighting scheme: what could you do other than using binary values"
      ],
      "metadata": {
        "id": "RvlDxUWKnU-S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Step 3 Processing (non binary)\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "def train_classifier_tfidf(data):\n",
        "    X = [t for (t, _) in data]\n",
        "    y = [y for (_, y) in data]\n",
        "\n",
        "    model = Pipeline([\n",
        "        ('vect', TfidfVectorizer(\n",
        "            tokenizer=pre_process,\n",
        "            # tokenizer=tokenize_lemmatize,\n",
        "            lowercase=False,\n",
        "            # ngram_range=(1, 2),\n",
        "            ngram_range=(1, 1),       # still unigrams here\n",
        "            sublinear_tf=True         # log scale TF\n",
        "        )),\n",
        "        ('svc', LinearSVC(random_state=42, class_weight='balanced'))\n",
        "    ])\n",
        "\n",
        "    model.fit(X, y)\n",
        "    return model\n",
        "# cross_validate(train_data, folds=10, classifier_fn=train_classifier_tfidf) # will work and output overall performance of p, r, f-score when cv implemented\n"
      ],
      "metadata": {
        "id": "PhGcMyjLatWF"
      },
      "execution_count": 221,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Step 3 Processing (words+char both)\n",
        "from sklearn.pipeline import FeatureUnion\n",
        "\n",
        "def train_classifier_char_and_words(data):\n",
        "    X = [t for (t, _) in data]\n",
        "    y = [y for (_, y) in data]\n",
        "\n",
        "    word_vect = TfidfVectorizer(\n",
        "        tokenizer=tokenize_lemmatize,\n",
        "        lowercase=False,\n",
        "        ngram_range=(1, 2),\n",
        "        sublinear_tf=True\n",
        "    )\n",
        "\n",
        "    char_vect = TfidfVectorizer(\n",
        "        analyzer='char',\n",
        "        ngram_range=(3, 5),     # 3-5 character n-grams\n",
        "        min_df=3\n",
        "    )\n",
        "\n",
        "    model = Pipeline([\n",
        "        ('features', FeatureUnion([\n",
        "            ('word', word_vect),\n",
        "            ('char', char_vect)\n",
        "        ])),\n",
        "        ('svc', LinearSVC(random_state=42, class_weight='balanced'))\n",
        "    ])\n",
        "\n",
        "    model.fit(X, y)\n",
        "    return model\n",
        "# cv_results = cross_validate(train_data, folds=10, classifier_fn=train_classifier_char_and_words)"
      ],
      "metadata": {
        "id": "xF_hamkPbC0U",
        "collapsed": true
      },
      "execution_count": 222,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 4 of Processing - You could add extra stylistic features like the number of words per sentence"
      ],
      "metadata": {
        "id": "rYOUUO0BnZnD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Step 4 Processing (Extra Stylistic Features)\n",
        "from sklearn.pipeline import Pipeline, FeatureUnion\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "from scipy.sparse import csr_matrix\n",
        "import numpy as np\n",
        "\n",
        "class StylisticFeatures(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        \"\"\"Extracts stylistic features from raw text X.\"\"\"\n",
        "        feature_values = []\n",
        "        for text in X:\n",
        "            sentences = text.split('.')  # naive sentence split\n",
        "            num_sentences = max(1, len(sentences))\n",
        "            num_words = len(text.split())\n",
        "            avg_words_per_sentence = num_words / num_sentences\n",
        "\n",
        "            feature_values.append([num_words, num_sentences, avg_words_per_sentence])\n",
        "\n",
        "        # convert into sparse matrix so it can be used in pipeline\n",
        "        return csr_matrix(np.array(feature_values))\n",
        "\n",
        "def train_classifier_with_style_feats(data):\n",
        "    X = [t for (t, _) in data]\n",
        "    y = [y for (_, y) in data]\n",
        "\n",
        "    word_vectorizer = TfidfVectorizer(\n",
        "        tokenizer=tokenize_lemmatize,\n",
        "        lowercase=False,\n",
        "        ngram_range=(1, 2),\n",
        "        sublinear_tf=True\n",
        "    )\n",
        "\n",
        "    # combine text features + stylistic features\n",
        "    combined_feats = FeatureUnion([\n",
        "        ('tfidf', word_vectorizer),\n",
        "        ('style', StylisticFeatures())\n",
        "    ])\n",
        "\n",
        "    model = Pipeline([\n",
        "        ('features', combined_feats),\n",
        "        ('svc', LinearSVC(random_state=42, class_weight= 'balanced'))\n",
        "    ])\n",
        "\n",
        "    model.fit(X, y)\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "rLTjkHiTbgz8"
      },
      "execution_count": 223,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# cv_results = cross_validate(train_data, folds=10, classifier_fn=train_classifier_with_style_feats)"
      ],
      "metadata": {
        "id": "80p7mGP7dled",
        "collapsed": true
      },
      "execution_count": 224,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 5 of Processing - You could consider playing with the parameters of the SVM cost parameter? per-class weighting?"
      ],
      "metadata": {
        "id": "TTOyLHS2nc94"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Step 5 Processing (SVM Parameters)\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "def train_classifier_svm_tuned(data, C=0.5):\n",
        "    X = [t for (t, _) in data]\n",
        "    y = [y for (_, y) in data]\n",
        "\n",
        "    model = Pipeline([\n",
        "        ('vect', TfidfVectorizer(\n",
        "            tokenizer=tokenize_lemmatize,\n",
        "            lowercase=False,\n",
        "            ngram_range=(1, 2),\n",
        "            sublinear_tf=True\n",
        "        )),\n",
        "        ('svc', LinearSVC(\n",
        "            C=C,                                 # cost parameter\n",
        "            class_weight='balanced',\n",
        "            random_state=42\n",
        "        ))\n",
        "    ])\n",
        "\n",
        "    model.fit(X, y)\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "nCRP5jCLdgNn"
      },
      "execution_count": 225,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# for C in [0.1, 0.5, 1.0, 2.0]:\n",
        "#     print(f\"\\nSVM cross-validation with C = {C}, class_weight = 'balanced'\")\n",
        "#     model_fn = lambda train_fold, C=C: train_classifier_svm_tuned(\n",
        "#         train_fold, C=C\n",
        "#     )\n",
        "#     scores = cross_validate(train_data, folds=10, classifier_fn=model_fn)\n",
        "#     print(\"F1 Score:\", scores[2])"
      ],
      "metadata": {
        "id": "WeJ4FEhYdjnW",
        "collapsed": true
      },
      "execution_count": 226,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 6 of Processing - You could do some feature selection, limiting the numbers of features through different controls on e.g. the vocabulary"
      ],
      "metadata": {
        "id": "HnvnaTArngvV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 6 Processing (Limiting no of features to 10k)\n",
        "def train_classifier_limited_vocab(data):\n",
        "    X = [t for (t, _) in data]\n",
        "    y = [y for (_, y) in data]\n",
        "\n",
        "    model = Pipeline([\n",
        "        ('vect', TfidfVectorizer(\n",
        "            tokenizer=tokenize_lemmatize,\n",
        "            lowercase=False,\n",
        "            ngram_range=(1, 2),\n",
        "            max_features=10000,       # limit features to top 10k\n",
        "            min_df=3,                 # minimum docs per term\n",
        "            max_df=0.9                # ignore too common terms\n",
        "        )),\n",
        "        ('svc', LinearSVC(random_state=42, class_weight='balanced'))\n",
        "    ])\n",
        "\n",
        "    model.fit(X, y)\n",
        "    return model\n",
        "# cross_validate(train_data, folds=10, classifier_fn=train_classifier_limited_vocab)"
      ],
      "metadata": {
        "id": "Iy5JEQaed7dp",
        "collapsed": true
      },
      "execution_count": 227,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 6 Processing (Keeping only k best features)\n",
        "from sklearn.feature_selection import SelectKBest, chi2\n",
        "\n",
        "def train_classifier_with_chi2(data, k):\n",
        "    X = [t for (t, _) in data]\n",
        "    y = [y for (_, y) in data]\n",
        "\n",
        "    model = Pipeline([\n",
        "        ('vect', TfidfVectorizer(\n",
        "            tokenizer=tokenize_lemmatize,\n",
        "            lowercase=False,\n",
        "            ngram_range=(1, 2)\n",
        "        )),\n",
        "        ('chi2', SelectKBest(chi2, k=k)),  # Keep only top k features\n",
        "        ('svc', LinearSVC(random_state=42, class_weight = 'balanced'))\n",
        "    ])\n",
        "\n",
        "    model.fit(X, y)\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "M-V76PcOd8ee"
      },
      "execution_count": 228,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# for k in [4000, 8000, 12000]:\n",
        "#     print(f\"\\nTesting Chi2 feature selection with k={k}\")\n",
        "#     model_fn = lambda train_fold, k=k: train_classifier_with_chi2(train_fold, k=k)\n",
        "#     cross_validate(train_data, folds=10, classifier_fn=model_fn)"
      ],
      "metadata": {
        "id": "c-pyvUIfeF5-",
        "collapsed": true
      },
      "execution_count": 229,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 7 of Processing - You could use external resources like the opinion lexicon available"
      ],
      "metadata": {
        "id": "09FCTN8VnsrO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 7 Processing (external resource opinion lexicon usage)\n",
        "#Please upload the positive-word and negative word text to the folder before running\n",
        "\n",
        "from sklearn.pipeline import Pipeline, FeatureUnion\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.svm import LinearSVC\n",
        "\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "from scipy.sparse import csr_matrix\n",
        "import numpy as np\n",
        "\n",
        "class OpinionLexiconFeatures(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self, pos_path, neg_path, tokenizer):\n",
        "        self.pos_path = pos_path\n",
        "        self.neg_path = neg_path\n",
        "        self.tokenizer = tokenizer\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        # load lexicons\n",
        "        self.pos_words = self._load_lexicon(self.pos_path)\n",
        "        self.neg_words = self._load_lexicon(self.neg_path)\n",
        "        return self\n",
        "\n",
        "    def _load_lexicon(self, path):\n",
        "        lex = set()\n",
        "        with open(path, 'r', encoding='latin-1') as f:\n",
        "            for line in f:\n",
        "                if not line.startswith(';') and line.strip():\n",
        "                    lex.add(line.strip())\n",
        "        return lex\n",
        "\n",
        "    def transform(self, X):\n",
        "        features = []\n",
        "        for text in X:\n",
        "            tokens = self.tokenizer(text)\n",
        "            pos_count = sum(1 for t in tokens if t in self.pos_words)\n",
        "            neg_count = sum(1 for t in tokens if t in self.neg_words)\n",
        "            sentiment_score = pos_count - neg_count\n",
        "            features.append([pos_count, neg_count, sentiment_score])\n",
        "\n",
        "        return csr_matrix(np.array(features))\n",
        "\n",
        "def train_classifier_with_lexicon(data):\n",
        "    X = [t for (t, _) in data]\n",
        "    y = [y for (_, y) in data]\n",
        "\n",
        "    word_tfidf = TfidfVectorizer(\n",
        "        tokenizer=tokenize_lemmatize,\n",
        "        lowercase=False,\n",
        "        ngram_range=(1, 2)\n",
        "    )\n",
        "\n",
        "    lexicon_feats = OpinionLexiconFeatures(\n",
        "        pos_path='positive-words.txt',\n",
        "        neg_path='negative-words.txt',\n",
        "        tokenizer=tokenize_lemmatize,\n",
        "    )\n",
        "\n",
        "    combined_feats = FeatureUnion([\n",
        "        ('tfidf', word_tfidf),\n",
        "        ('lexicon', lexicon_feats)\n",
        "    ])\n",
        "\n",
        "    model = Pipeline([\n",
        "        ('features', combined_feats),\n",
        "        ('svc', LinearSVC(random_state=42, class_weight='balanced'))\n",
        "    ])\n",
        "\n",
        "    model.fit(X, y)\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "Kucv2Do5ewQc"
      },
      "execution_count": 230,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# results = cross_validate(train_data, folds=10, classifier_fn=train_classifier_with_lexicon)\n",
        "# print(results)\n"
      ],
      "metadata": {
        "id": "dAh5g5sie0KV",
        "collapsed": true
      },
      "execution_count": 231,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Testing the most effective combinations from the above to achieve the best model"
      ],
      "metadata": {
        "id": "uVk-czeamhDL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# -----------------------------\n",
        "# Best classifier (word + char TF-IDF + lexicon + chi2 + LinearSVC)\n",
        "# -----------------------------\n",
        "import os\n",
        "import numpy as np\n",
        "from scipy.sparse import csr_matrix, hstack\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "from sklearn.pipeline import Pipeline, FeatureUnion\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.feature_selection import SelectKBest, chi2\n",
        "from sklearn.svm import LinearSVC\n",
        "\n",
        "# -----------------------------\n",
        "# Safe chi2 selector (won't crash if k > #features)\n",
        "# -----------------------------\n",
        "class SafeSelectKBest(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self, score_func=chi2, k=12000):\n",
        "        self.score_func = score_func\n",
        "        self.k = k\n",
        "        self.selector_ = None\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        k_use = self.k\n",
        "        if isinstance(k_use, int):\n",
        "            k_use = min(k_use, X.shape[1])  # clamp to available features\n",
        "        self.selector_ = SelectKBest(self.score_func, k=k_use).fit(X, y)\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        return self.selector_.transform(X)\n",
        "\n",
        "# -----------------------------\n",
        "# Opinion Lexicon transformer (always non-negative outputs for chi2)\n",
        "# features: [pos_count, neg_count, pos_plus_neg]\n",
        "# -----------------------------\n",
        "class OpinionLexiconFeatures(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self, pos_path='positive-words.txt', neg_path='negative-words.txt', tokenizer=None):\n",
        "        self.pos_path = pos_path\n",
        "        self.neg_path = neg_path\n",
        "        self.tokenizer = tokenizer\n",
        "        self.pos_words = None\n",
        "        self.neg_words = None\n",
        "\n",
        "    def _load(self, path):\n",
        "        if not os.path.exists(path):\n",
        "            return None\n",
        "        words = []\n",
        "        with open(path, encoding='latin-1') as f:\n",
        "            for line in f:\n",
        "                w = line.strip()\n",
        "                if not w or w.startswith(';'):\n",
        "                    continue\n",
        "                words.append(w.lower())\n",
        "        return set(words)\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        self.pos_words = self._load(self.pos_path)\n",
        "        self.neg_words = self._load(self.neg_path)\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        # if files missing or no tokenizer, return zeros (3 cols)\n",
        "        if self.pos_words is None or self.neg_words is None or self.tokenizer is None:\n",
        "            Z = np.zeros((len(X), 3), dtype=np.float32)\n",
        "            return csr_matrix(Z)\n",
        "\n",
        "        rows = []\n",
        "        for text in X:\n",
        "            toks = self.tokenizer(text)\n",
        "            pc = sum(1 for t in toks if t in self.pos_words)\n",
        "            nc = sum(1 for t in toks if t in self.neg_words)\n",
        "            rows.append([pc, nc, pc + nc])   # <-- all non-negative\n",
        "        return csr_matrix(np.asarray(rows, dtype=np.float32))\n",
        "\n",
        "# -----------------------------\n",
        "# Best classifier (word + char TF-IDF + lexicon + chi2 + LinearSVC)\n",
        "# -----------------------------\n",
        "def best_classifier(train_fold, k=12000, C=0.92):\n",
        "    X = [t for (t, _) in train_fold]\n",
        "    y = [y for (_, y) in train_fold]\n",
        "\n",
        "    word_vect = TfidfVectorizer(\n",
        "        tokenizer=tokenize_lemmatize,\n",
        "        lowercase=False,\n",
        "        ngram_range=(1, 2),\n",
        "        min_df=2,\n",
        "        max_df=1.0,\n",
        "        sublinear_tf=True,\n",
        "        dtype=np.float32\n",
        "    )\n",
        "\n",
        "    char_vect = TfidfVectorizer(\n",
        "        analyzer='char',\n",
        "        ngram_range=(3, 5),\n",
        "        min_df=4,\n",
        "        sublinear_tf=True,\n",
        "        dtype=np.float32\n",
        "    )\n",
        "\n",
        "    lexicon_feats = OpinionLexiconFeatures(\n",
        "        pos_path='positive-words.txt',\n",
        "        neg_path='negative-words.txt',\n",
        "        tokenizer=tokenize_lemmatize\n",
        "    )\n",
        "\n",
        "    combined_feats = FeatureUnion([\n",
        "        ('word', word_vect),\n",
        "        ('char', char_vect),\n",
        "        ('lexicon', lexicon_feats),\n",
        "    ])\n",
        "\n",
        "    model = Pipeline([\n",
        "        ('features', combined_feats),\n",
        "        ('chi2', SelectKBest(chi2, k=k)),                 # supervised selection (non-negative only)\n",
        "        ('svc', LinearSVC(C=C, random_state=42, class_weight='balanced'))\n",
        "    ])\n",
        "\n",
        "    model.fit(X, y)\n",
        "    return model\n",
        "\n",
        "# scores = cross_validate(train_data, folds=10, classifier_fn=best_classifier)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "lsTNtvfBvM60"
      },
      "execution_count": 232,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 233,
      "metadata": {
        "id": "V9pNlabIdSFB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6e413a7a-0c7e-4e8f-ffaf-d03bbc429d17"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('@chaddockr @LexStarwalker Your conversation on race/species in the #CypherSystem gave me much to think about. I may make a post on Google+.', 'positive')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Done training!\n",
            "Precision: 0.890512\n",
            "Recall: 0.888939\n",
            "F Score:0.889495\n"
          ]
        }
      ],
      "source": [
        "# Finally, check the accuracy of your classifier by training on all the traning data\n",
        "# and testing on the test set\n",
        "# Will only work once all functions are complete\n",
        "from sklearn import metrics\n",
        "functions_complete = True  # set to True once you're happy with your methods for cross val\n",
        "if functions_complete:\n",
        "    print(test_data[0])   # have a look at the first test data instance\n",
        "    classifier = best_classifier(train_data)  # train the classifier\n",
        "    test_true = [t[1] for t in test_data]   # get the ground-truth labels from the data\n",
        "    test_pred = predict_labels([x[0] for x in test_data], classifier)  # classify the test data to get predicted labels\n",
        "    final_scores = precision_recall_fscore_support(test_true, test_pred, average='weighted') # evaluate\n",
        "    print(\"Done training!\")\n",
        "    print(\"Precision: %f\\nRecall: %f\\nF Score:%f\" % final_scores[:3])\n",
        "\n",
        "    # # Confusion matrix + classification report\n",
        "    # print(\"\\nConfusion Matrix:\")\n",
        "    # print(metrics.confusion_matrix(test_true, test_pred))\n",
        "    # print(\"\\nClassification Report:\")\n",
        "    # print(classification_report(test_true, test_pred))\n",
        "\n",
        "    # # Identify false positives and false negatives for positive label\n",
        "    # false_positives = []\n",
        "    # false_negatives = []\n",
        "\n",
        "    # for text, true_label, pred_label in zip(test_data, test_true, test_pred):\n",
        "    #     if true_label == \"negative\" and pred_label == \"positive\":\n",
        "    #         false_positives.append(text)\n",
        "    #     elif true_label == \"positive\" and pred_label == \"negative\":\n",
        "    #         false_negatives.append(text)\n",
        "\n",
        "    # Print a few examples\n",
        "    # print(f\"\\nFalse Positives ({len(false_positives)}):\")\n",
        "    # for fp in false_positives[:10]:\n",
        "    #     print(\"→\", fp)\n",
        "\n",
        "    # print(f\"\\nFalse Negatives ({len(false_negatives)}):\")\n",
        "    # for fn in false_negatives[:10]:\n",
        "    #     print(\"→\", fn)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.3"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
